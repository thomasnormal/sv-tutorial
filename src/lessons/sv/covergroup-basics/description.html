<p>Assertions tell you when something goes <em>wrong</em>. Coverage tells you whether you've <em>tested enough</em>. After 20 random cycles against our SRAM, have we actually hit all 16 addresses? Did we do both reads and writes?</p>
<p>A <code><dfn data-card="A covergroup is a SystemVerilog construct that defines a set of things to measure during simulation. It is instantiated like a class object and sampled at a trigger event (typically a clock edge). The tool tracks which values have been seen and reports a coverage percentage at the end of the run.">covergroup</dfn></code> defines what to measure. Each <code><dfn data-card="A coverpoint names a signal or expression to track inside a covergroup. By default, the tool auto-creates one bin per possible value. You can add explicit named bins to group values into categories that are meaningful for verification (e.g., 'all addresses in the upper half' rather than tracking each address individually).">coverpoint</dfn></code> inside it names a signal to track. At each sampling event the tool records which value was seen and updates a set of <strong><dfn data-card="A coverage bin represents one category of values that a coverpoint tracks. By default, SystemVerilog auto-creates one bin per possible value (an 8-bit signal gets 256 bins). Named bins let you group values into meaningful categories: 'low half' covers 0–7, 'high half' covers 8–15. A bin is 'hit' once its value appears at least once during simulation. Coverage percentage = (bins hit) / (total bins). ignore_bins excludes values from the count; illegal_bins additionally causes a simulation error if the value is ever seen.">bins</dfn></strong> — one per distinct value by default:</p>
<pre>covergroup cg @(posedge clk);
  cp_sig: coverpoint sig;  // auto-creates one bin per possible value
endgroup

cg cov = new;  // instantiate like a class object</pre>
<p>Open <code>cov_intro.sv</code>. The SRAM and its random stimulus are already provided. Add the <code>covergroup</code>, two <code>coverpoint</code>s (one for <code>addr</code>, one for <code>we</code>), and instantiate it. Run and observe the coverage percentage: with only 20 random cycles, are all 16 addresses hit?</p>
<blockquote><p>Coverage percentage tells you which values were <em>seen</em> — not whether the design behaved correctly for each one. You need both assertions (correctness) and coverage (completeness) for a thorough verification plan.</p></blockquote>
